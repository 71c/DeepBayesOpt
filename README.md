# Model-free Offline RL Bayesian Optimization

This repository contains code for research on using offline deep reinforcement learning techniques to learn acquisition functions for Bayesian optimization. Instead of relying on surrogate models like Gaussian Processes, this approach trains neural networks end-to-end on datasets of objective functions to approximate acquisition functions such as Expected Improvement and the Gittins index.

The repository includes:
- Scripts for generating synthetic datasets of black-box objective functions (GP-based, logistic regression, and HPO-B benchmark), training neural network-based acquisition functions, and running Bayesian optimization loops.
- Fully automated running of experiments using YAML config files and command-line Python scripts to automate the submission of several dependent jobs and job arrays using SLURM.
- Centralized experiment management system with a registry for tracking and organizing complex experiments.
- Tools assessing the Bayesian optimization performance through scripts that generate and save plots in a hierarchical, systematic way.

NOTE: I might not update this README as quickly as I change the code.


# Installation

To install the required packages, make a new Python environment, for example using Anaconda:
```bash
conda create --name nn_bo python=3.12.4
```
(I'm running the code on both Python 3.12.4 and 3.9.12, they both work.)

Next, switch to the new environment:
```bash
conda activate nn_bo
```
Then install the required packages using pip:
```bash
pip install -r requirements.txt
```

## Activating the Environment

A convenience script is provided to activate the conda environment:
```bash
source ./bin/activate_env.sh
```
This script will automatically detect the conda installation and activate the `nn_bo` environment. Note that you must use `source` (or `.`) to run the script, not `./`, so that the activation persists in your current shell.

The `nn_bo` environment is also automatically activated in SLURM jobs via `utils_general/experiments/job_array.sub`.

# Command-line scripts overview
The command-line scripts are organized as follows. See the following sections for more details on each script.

**Recommended Entry Point:**
- **Experiment Manager:** `bin/experiment_manager.py` - Centralized CLI for managing experiments through the experiment registry system (see [Experiment Manager CLI](#experiment-manager-cli) section).

**Core Scripts (can be called directly):**
- **BO loops (+ NN training):** `single_run.py` for running a single BO loop, `submit.py` for running multiple. `submit.py` is the most high-level script.
- **NN training (+ dataset generation):** `single_train.py` for training a single NN, `submit_train.py` for training multiple.
- **Dataset generation:** Datasets are automatically generated by dataset managers in `datasets/` directory. See the `dataset_factory.py` for the unified interface.
- **Making plots:** `plot_run.py` for BO experiment plots, `plot_train.py` for training plots, `plot_combined.py` for combined plots.
- **Status checking:** `status.py` for checking experiment status.
- **Other utilities:** `get_training_stats.py` for training statistics, `plot_1d_objectives.py` for 1D objective visualizations, `g2top.py` for GPU monitoring.


# Experiment Manager CLI

The `bin/experiment_manager.py` script provides a centralized command-line interface for managing experiments through the experiment registry system. It offers a higher-level alternative to directly calling `submit.py`, `status.py`, and `plot_run.py`.

## Basic Usage

Run `python bin/experiment_manager.py --help` to see all available commands.

### List Available Experiments
```bash
python bin/experiment_manager.py list
```

### List Available Templates
```bash
python bin/experiment_manager.py list-templates
```

### Show Experiment Details
```bash
# Show configuration
python bin/experiment_manager.py show <experiment_name>

# Show commands that would be executed
python bin/experiment_manager.py show <experiment_name> --commands
```

### Check Experiment Status
```bash
# Check status of a specific experiment
python bin/experiment_manager.py status <experiment_name>

# Check status of all experiments
python bin/experiment_manager.py status

# Dry run (show what would be checked without executing)
python bin/experiment_manager.py status <experiment_name> --dry-run
```

### Run Experiments
```bash
# Run a full experiment (training + BO loops)
python bin/experiment_manager.py run <experiment_name>

# Run only the training part
python bin/experiment_manager.py run <experiment_name> --training-only

# Dry run (show what would be executed without running)
python bin/experiment_manager.py run <experiment_name> --dry-run

# Prepare jobs but don't submit to SLURM
python bin/experiment_manager.py run <experiment_name> --no-submit
```

#### Recompute Options
```bash
# Recompute/overwrite existing NN training results
python bin/experiment_manager.py run <experiment_name> --always-train

# Recompute/overwrite all existing BO results
python bin/experiment_manager.py run <experiment_name> --recompute-run

# Recompute/overwrite only non-NN BO results (GP and random search)
python bin/experiment_manager.py run <experiment_name> --recompute-non-train-only
```

Note: `--recompute-run` and `--recompute-non-train-only` are mutually exclusive.

### Generate Plots
```bash
# Generate default BO experiment plots
python bin/experiment_manager.py plot <experiment_name>

# Generate training plots
python bin/experiment_manager.py plot <experiment_name> --type train_plot

# Use a specific plot variant
python bin/experiment_manager.py plot <experiment_name> --variant compact

# Customize plot appearance
python bin/experiment_manager.py plot <experiment_name> --center-stat median --add-grid --add-markers

# Limit the number of iterations displayed
python bin/experiment_manager.py plot <experiment_name> --max-iterations-to-plot 20
```

### List Plot Variants
```bash
# List all available plot variants for an experiment
python bin/experiment_manager.py plot-variants <experiment_name>

# List variants for a specific plot type
python bin/experiment_manager.py plot-variants <experiment_name> --type run_plot
```

### Show Commands
```bash
# Display the commands that would be executed (similar to commands.txt)
python bin/experiment_manager.py commands <experiment_name>
```

## Advantages of Using the Experiment Manager

1. **Centralized Configuration**: Experiments are defined in `experiments/registry.py` with structured configurations
2. **Simpler Commands**: No need to specify multiple config files and parameters each time
3. **Consistency**: Ensures experiments are run with validated, versioned configurations
4. **Documentation**: Each experiment includes a description and creation date
5. **Plot Management**: Multiple plot variants can be defined and easily selected
6. **Status Tracking**: Built-in status checking across all experiments

## Example Workflow

```bash
# List available experiments
python bin/experiment_manager.py list

# Show what an experiment will do
python bin/experiment_manager.py show my_experiment --commands

# Run the experiment
python bin/experiment_manager.py run my_experiment

# Check status
python bin/experiment_manager.py status my_experiment

# Generate plots once complete
python bin/experiment_manager.py plot my_experiment

# Try different plot variants
python bin/experiment_manager.py plot-variants my_experiment
python bin/experiment_manager.py plot my_experiment --variant compact
```


# BO loops (+ NN training if necessary)
Use `single_run.py` for running a single BO loop, and use `submit.py` for running multiple. `submit.py` is the most high-level script and the one that is most likely to be used.

## Running a single Bayesian optimization loop
To run a BO loop, you can use `single_run.py`. `single_run.py --help` will show the description of the arguments. If the NN model has not been trained yet, it will raise an error. In this case, you need to enter the command to train the NN model and then the command to run the BO loop.

Example commands:
- Random Search:
```bash
python single_run.py --n_initial_samples 1 --n_iter 20 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --random_search --bo_seed 6888556634303915349 --objective_dataset_type gp --objective_id 6888556634303915349

python single_run.py --n_initial_samples 1 --n_iter 20 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --random_search --bo_seed 8643049736318478698 --objective_dataset_type gp --objective_id 8643049736318478698
```
- GP-based acquisition function:
```bash
python single_run.py --n_initial_samples 1 --n_iter 20 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --gp_af LogEI --gp_af_fit exact --num_restarts 160 --raw_samples 3200 --gen_candidates L-BFGS-B --bo_seed 6888556634303915349 --objective_dataset_type gp --objective_id 6888556634303915349

python single_run.py --n_initial_samples 1 --n_iter 20 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --gp_af LogEI --gp_af_fit exact --num_restarts 160 --raw_samples 3200 --gen_candidates L-BFGS-B --bo_seed 8643049736318478698 --objective_dataset_type gp --objective_id 8643049736318478698

python single_run.py --eps 1e-08 --ftol 2.220446049250313e-09 --gen_candidates L-BFGS-B --gp_af gittins --gp_af_fit exact --gtol 1e-05 --lamda 0.01 --maxcor 10 --maxfun 15000 --maxiter 15000 --maxls 20 --n_initial_samples 1 --n_iter 20 --num_restarts 160 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --raw_samples 3200 --bo_seed 6888556634303915349 --objective_dataset_type gp --objective_id 6888556634303915349

python single_run.py --eps 1e-08 --ftol 2.220446049250313e-09 --gen_candidates L-BFGS-B --gp_af gittins --gp_af_fit exact --gtol 1e-05 --lamda 0.01 --maxcor 10 --maxfun 15000 --maxiter 15000 --maxls 20 --n_initial_samples 1 --n_iter 20 --num_restarts 160 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --raw_samples 3200 --bo_seed 8643049736318478698 --objective_dataset_type gp --objective_id 8643049736318478698
```
- NN-based acquisition function:
```bash
python single_run.py --lamda 1e-2 --n_initial_samples 1 --n_iter 20 --nn_model_name v2/model_35f043d1473e2adc8a97027e56c8dc8cefd60ef48a14382cfd07e60e52a55234 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --num_restarts 160 --raw_samples 3200 --gen_candidates L-BFGS-B --bo_seed 6888556634303915349 --objective_dataset_type gp --objective_id 6888556634303915349

python single_run.py --lamda 1e-2 --n_initial_samples 1 --n_iter 20 --nn_model_name v2/model_35f043d1473e2adc8a97027e56c8dc8cefd60ef48a14382cfd07e60e52a55234 --objective_dimension 1 --objective_kernel Matern52 --objective_lengthscale 0.05 --num_restarts 160 --raw_samples 3200 --gen_candidates L-BFGS-B --bo_seed 8643049736318478698 --objective_dataset_type gp --objective_id 8643049736318478698
```
See the [section on NN training](#nn-training--dataset-generation-if-necessary) for how to train the NN model and obtain `--nn_model_name`.

For `--gen_candidates` when optimizing AFs, you can currently choose between "L-BFGS-B" and "torch".


## Running multiple Bayesian optimization loops
The following command automatically runs all of the BO loops of both the NNs and the GP-based AFs. Run `python submit.py --help` to see the description of the arguments. Unlike the command for running a single BO loop, this command will automatically train any NNs that have not been trained yet prior to optimizing with them.

An example command is as follows:
```bash
python submit.py --train_base_config config/train_acqf.yml --train_experiment_config config/train_acqf_experiment_test_simple.yml --run_base_config config/bo_config.yml --n_gp_draws 8 --seed 8 --sweep_name preliminary-test-small --mail adj53@cornell.edu --gres gpu:1
```

### Arguments
#### Objective functions and seed
- `--n_gp_draws`: the number of draws of GP objective functions per set of GP params.
- `--seed SEED`: the seed for the random number generator.

#### BO loop
- `--n_iter`: the number of iterations of BO to perform
- `--n_initial_samples`: the number of initial sobol points to sample at before using the AF.

#### NN training experiments
- `--train_base_config` is the base configuration file, containing the default values and default ranges to search over for all of the hyperparameters.
- `--train_experiment_config` is the experiment configuration file, containing the specific values and ranges to search over a subset of the hyperparameters for the particular experiment. Replace the value for `--train_experiment_config` with your desired experiment configuration file.  For example, to investigate the effect of the hyperparameters regarding the dataset, NN architecture, and optimizer settings, we can specify the experiment configuration file to be `config/train_acqf_experiment_training.yml`, which varies `train_samples_size`, `layer_width`, and `learning_rate`, while fixing the dimension to 16 and the method to Gittins index with $\lambda=10^{-4}$.
Alternatively, you can use `config/train_acqf_experiment_test_simple.yml` to just run a single NN training.
- `--always_train`: If this flag is set, train all acquisition function NNs regardless of whether they have already been trained. Default is to only train acquisition function NNs that have not already been trained.

#### SLURM-based job submission
- `--sweep_name` is the name of the "sweep" (in Weights and Biases terminology). In this case, it just corresponds to the name of the directory where the err and out files, and other information about the experiment submission, will be saved.
- `--mail` is the email address to send a notification to when the job is done (optional).
- `--gres` is the GPU resource to request. In this case, it is requesting one GPU. (Also optional.)

Other arguments like partition and time may be added to the script if necessary.


# NN training (+ dataset generation if necessary)
Although the NN training is automatically done when running the BO loops, you can also just train the all the NNs without doing anything with them just yet, with the following scripts.
Use `single_train.py` for training a single NN, and `submit_train.py` for training multiple.

## Training a single neural network
`single_train.py` is the script that trains a single neural network. `single_train.py --help` will show the description of the arguments. An example command is as follows:
**GP Dataset Example:**
```bash
python single_train.py --dataset_type gp --dimension 1 --lengthscale 0.05 --kernel Matern52 --min_history 1 --max_history 20 --replacement --train_n_candidates 1 --test_n_candidates 1 --train_acquisition_size 8192 --train_samples_size 10000 --test_expansion_factor 1 --test_samples_size 5000 --batch_size 512 --early_stopping --min_delta 0.0 --patience 30 --layer_width 200 --learning_rate 3e-4 --lr_scheduler ReduceLROnPlateau --lr_scheduler_patience 15 --lr_scheduler_factor 0.1 --method gittins --lamda 1e-2 --gi_loss_normalization normal --architecture pointnet --epochs 3
```

**Logistic Regression Dataset Example:**
```bash
python single_train.py --dataset_type logistic_regression --train_samples_size 5000 --test_samples_size 2000 --train_acquisition_size 8000 --batch_size 128 --epochs 200 --layer_width 300 --learning_rate 3e-4 --method gittins --lamda 1e-2 --architecture pointnet --train_n_candidates 5 --test_n_candidates 10 --min_history 1 --max_history 50 --lr_n_samples_range 100 1000 --lr_n_features_range 10 100 --lr_log_lambda_range -6 2 --early_stopping --patience 30
```

**HPO-B Dataset Example:**
```bash
python single_train.py --dataset_type hpob --hpob_search_space_id 5970 --min_history 1 --max_history 20 --train_acquisition_size 8000 --batch_size 128 --epochs 4000 --layer_width 16 --learning_rate 3e-4 --method gittins --lamda 1e-2 --architecture pointnet
```
It will output
```
Saving model and configs to v2/model_35f043d1473e2adc8a97027e56c8dc8cefd60ef48a14382cfd07e60e52a55234
```
This identifies the neural network model, which is uniquely identified by the specific combination of dataset, architecture, training method, optimizer settings, etc. This information, along with the weights of the NN corresponding to the epoch where it performed the best on the test dataset, are saved to this directory.

Example of a long-running command:
```bash
python single_train.py --dimension 1 --kernel Matern52 --lamda 0.01 --lengthscale 0.05 --max_history 20 --min_history 1 --replacement --test_expansion_factor 1 --test_n_candidates 1 --test_samples_size 10000 --train_acquisition_size 30000 --train_n_candidates 1 --train_samples_size 10000 --batch_size 512 --early_stopping --epochs 500 --gi_loss_normalization normal --lamda 0.01 --layer_width 300 --learning_rate 0.001 --lr_scheduler ReduceLROnPlateau --lr_scheduler_cooldown 0 --lr_scheduler_factor 0.1 --lr_scheduler_min_lr 0.0 --lr_scheduler_patience 15 --method gittins --min_delta 0.0 --patience 30 --architecture pointnet
```

### Dataset generation
In order to train the neural network, you need to have a dataset of black-box objective functions. The codebase supports multiple dataset types:

1. **Gaussian Process (GP)**: Randomly generated synthetic functions using GP models (default)
2. **Logistic Regression**: Hyperparameter optimization tasks for regularized logistic regression
3. **HPO-B**: Real-world hyperparameter optimization benchmarks from the HPO-B dataset
4. **Cancer Dosage**: Cancer dosage optimization tasks

Since it takes some time to generate the datasets, they are cached in the `data/datasets/` directory. When running `single_train.py`, if the dataset is not found, it will automatically generate the dataset and save it in the `data/datasets/` directory.

Dataset generation is handled automatically by the dataset managers in the `datasets/` directory:
- `datasets/gp_acquisition_dataset_manager.py`: Manages GP-based datasets
- `datasets/lr_acquisition_dataset_manager.py`: Manages logistic regression datasets
- `datasets/hpob_acquisition_dataset_manager.py`: Manages HPO-B datasets
- `datasets/cancer_dosage_acquisition_dataset_manager.py`: Manages cancer dosage datasets

The `dataset_factory.py` provides a unified interface for creating datasets of different types. Dataset caching uses content-based naming to avoid regenerating identical datasets across experiments.


## Training multiple neural networks
`python submit_train.py` is the script that trains multiple neural networks.
Run `python submit_train.py --help` for the description of the arguments.
An example command is as follows:
```bash
python submit_train.py --train_base_config config/train_acqf.yml --train_experiment_config config/train_acqf_experiment_test_simple.yml --sweep_name preliminary-test-small-train --mail adj53@cornell.edu --gres gpu:1
```
This will train multiple neural networks with different hyperparameters and save the models.


# Making plots
`plot_run.py` will make the plots. You can run `python plot_run.py --help` for a description of the arguments. As opposed to either throwing an error or automatically running the prerequisite commands as in the other scripts, this script will only generate plots for the BO loops that have already been run.

An example command is as follows:
```bash
python plot_run.py --train_base_config config/train_acqf.yml --train_experiment_config config/train_acqf_experiment_1dim_example.yml --run_base_config config/bo_config.yml --run_experiment_config config/bo_config_experiment_2_20iter_160.yml --n_gp_draws 2 --seed 8 --use_rows --use_cols --center_stat mean --interval_of_center --plots_group_name test_1dim_maxhistory20_example --plots_name results_20iter
```

## Plot Formatting Options

The plotting script supports several formatting options to improve plot readability:

- `--add_grid`: Add a grid to the plots for easier reading
- `--add_markers`: Add markers to the lines at each iteration point
- `--min_regret_for_plot`: Set the minimum regret value displayed in log-scale plots (default: 1e-6). Values below this threshold will be clipped to prevent extremely small regrets from compressing the y-axis range. For example, use `--min_regret_for_plot 1e-8` to allow smaller regret values to be displayed.
- `--max_iterations_to_plot`: Manually specify the maximum number of BO iterations to display in plots. If not specified, the script will automatically determine an optimal value when plotting regret metrics.

### Automatic Max Iterations Detection

When plotting regret metrics (`normalized_regret` or `regret`) without specifying `--max_iterations_to_plot`, the script automatically determines an optimal number of iterations to display. This feature:

1. Analyzes all BO methods to find which ones achieve regret below the threshold (specified by `--min_regret_for_plot`)
2. Identifies the maximum number of iterations required across all methods to reach the threshold
3. Adds a buffer (default: 25% more iterations, minimum 5) to show convergence stability
4. Focuses the plot on the "interesting" convergence region while avoiding excessive flat regions

You can control the auto-detection behavior with these parameters:
- `--auto_max_iterations_buffer`: Fraction of extra iterations to add as buffer (default: 0.25 for 25%)
- `--auto_max_iterations_min_buffer`: Minimum number of iterations to add as buffer (default: 5)

Example with formatting options and auto-detection:
```bash
python plot_run.py --train_base_config config/train_acqf.yml --train_experiment_config config/train_acqf_experiment_1dim_example.yml --run_base_config config/bo_config.yml --n_gp_draws 2 --seed 8 --use_rows --use_cols --add_grid --add_markers --min_regret_for_plot 1e-8 --auto_max_iterations_buffer 0.3 --plots_group_name test_plots --plots_name results
```

Example with manual max iterations (disables auto-detection):
```bash
python plot_run.py --train_base_config config/train_acqf.yml --train_experiment_config config/train_acqf_experiment_1dim_example.yml --run_base_config config/bo_config.yml --n_gp_draws 2 --seed 8 --max_iterations_to_plot 50 --plots_group_name test_plots --plots_name results
```
This means that at the highest level it will vary the layer width and the training samples size, then the lambda, the GP acquisition function or NN method, and finally the seed for the GP. The GP seed corresponds to individual BO runs that together comprise an error bar that is in the legend of a specific subplot. The higher levels make up subplots within a figure, figures (which correspond to `.pdf` files), and folders containing the figures. 
The script will output something like the following to indicate to the user the structure of the plots:
```
  folder: {'dimension'}
  fname: {'nn.train_samples_size', 'nn.layer_width'}
  line: {'nn.gi_loss_normalization', 'lamda', 'nn.lamda_min', 'nn.learning_rate', 'gp_af', 'nn.lamda_max', 'nn.method', 'nn.lamda'}
  random: {'objective.gp_seed'}
```
To do this, looks at the attributes that were not already specified in `config/plots_config_1.yml` but that do vary in the experiment. Of these, of the ones that are always present across all experiments, it adds them to a new, topmost level. And the rest, it adds to the existing second-to-last level (the "line" level).

On the other hand, if you run the same command above but additionally with the flag `--use_cols` (you can also enable `--use_cols` instead of or in addition to `--use_rows`), then the script will output the following:
```
  fname: {'dimension'}
  col: {'nn.layer_width', 'nn.train_samples_size'}
  line: {'nn.lamda_min', 'nn.method', 'nn.lamda_max', 'lamda', 'nn.lamda', 'gp_af', 'nn.gi_loss_normalization', 'nn.learning_rate'}
  random: {'objective.gp_seed'}
```
This means that each figure (file) corresponds to its own dimension, each subplot with in a figure corresponds to a combination of layer width and training samples size, error bar ("line") corresponds to a different BO policy, and the seed is the source of randomness for the error bar.
Enabling `--use_rows` and/or `use_cols` is more compact since it fits multiple subplots in a single figure so you can view them side by side.


# Overview of the rest of the codebase

## Datasets
- `datasets/`: Directory containing dataset implementations:
  - `function_samples_dataset.py`: Defines classes that represent samples of functions. Uses `utils_general/dataset_with_models.py` to create the class `FunctionSamplesDataset`, along with `MapFunctionSamplesDataset`, `ListMapFunctionSamplesDataset`, `LazyMapFunctionSamplesDataset`, and `MapFunctionSamplesSubset`. Also defines `TransformedFunctionSamplesIterableDataset`, `TransformedLazyMapFunctionSamplesDataset`, `GaussianProcessRandomDataset`, and `ResizedFunctionSamplesIterableDataset`.
  - `acquisition_dataset.py`: Defines classes that represent datasets for training acquisition functions. Uses `function_samples_dataset.py` to create the class `AcquisitionDataset`, along with `MapAcquisitionDataset`, `ListMapAcquisitionDataset`, `LazyMapAcquisitionDataset`, and `MapAcquisitionSubset`. Defines `FunctionSamplesAcquisitionDataset` which is used to create an acquisition dataset from a function samples dataset. Also defines `CostAwareAcquisitionDataset` which simply combines random $\lambda$ values with the acquisition dataset.
  - `hpob_dataset.py`: Implementation for HPO-B benchmark datasets.
  - `logistic_regression_dataset.py`: Implementation for logistic regression hyperparameter optimization datasets.
  - `cancer_dosage_dataset.py`: Implementation for cancer dosage optimization datasets.
  - `utils.py`: Utility functions for dataset operations.

## Dataset Management
- `dataset_factory.py`: Unified factory interface for creating different dataset types.
- `datasets/acquisition_dataset_manager.py`: Base classes and manager architecture for acquisition datasets.
- `datasets/gp_acquisition_dataset_manager.py`: Manager for GP-based datasets.
- `datasets/lr_acquisition_dataset_manager.py`: Manager for logistic regression datasets.
- `datasets/hpob_acquisition_dataset_manager.py`: Manager for HPO-B datasets.
- `datasets/cancer_dosage_acquisition_dataset_manager.py`: Manager for cancer dosage datasets.

## Experiment Management
- `experiments/`: Directory containing the centralized experiment management system:
  - `registry.py`: Central registry for managing experiment configurations.
  - `runner.py`: Experiment execution and orchestration.
  - `registry.yml`: YAML file containing all experiment configurations.
  - `README.md`: Documentation for the experiment management system.
- `bin/experiment_manager.py`: High-level CLI for running registered experiments.
- `utils_general/experiments/`: General-purpose experiment utilities:
  - `experiment_config_utils.py`: Configuration management.
  - `experiment_manager.py`: Experiment manager utilities.
  - `registry.py`: General-purpose experiment registry.
  - `runner.py`: General-purpose experiment runner.
  - `submit_dependent_jobs.py`: Job dependency handling.
  - `job_array.sub`: SLURM job array template.

## NN AF Architecture: `acquisition_function_net.py`
`acquisition_function_net.py` defines PyTorch modules for acquisition functions in likelihood-free Bayesian optimization. It includes modular classes for structured acquisition function design, such as:
- `AcquisitionFunctionNet`: Base class for acquisition function networks.
- `ParameterizedAcquisitionFunctionNet`: Supports acquisition functions with parameters.
- `TwoPartAcquisitionFunctionNet`: Splits acquisition functions into a feature-extracting body and an MLP head.
- `GittinsAcquisitionFunctionNet`: Implements Gittins index-based acquisition function functionality.
- `ExpectedImprovementAcquisitionFunctionNet`: Implements the architecture and interface used for the neural-network-based expected improvement.
- `AcquisitionFunctionNetModel` and `AcquisitionFunctionNetAcquisitionFunction`: Enable integration with BoTorch to be used in Bayesian optimization loops.

## Code for NN Training: `train_acquisition_function_net.py`
`train_acquisition_function_net.py` contains the code for training neural networks for acquisition functions. In particular, the function `train_acquisition_function_net` is only used in `single_train.py`. Additionally, this script defines functions to be used for loading and saving the NN models.

## Code for running BO loops: `bayesopt.py`
This module implements the core Bayesian optimization loop. It includes:

- **Optimizer Classes:**  
  - **`BayesianOptimizer`**: An abstract base class handling initialization, tracking of the best observed values, and time statistics.  
  - **`RandomSearch`**: A simple random sampling baseline.  
  - **`SimpleAcquisitionOptimizer`**: Uses an acquisition function (optimized via BoTorch) to select new evaluation points.  
  - **`ModelAcquisitionOptimizer`**: Integrates a model (GP or NN) with the acquisition function.  
  - **`NNAcquisitionOptimizer` / `GPAcquisitionOptimizer`**: Concrete implementations for NN-based and GP-based acquisition functions, respectively.

- **Results & Experiment Management:**
  Classes for caching, saving, and validating BO results across functions and trials. This includes `OptimizationResultsSingleMethod` and `OptimizationResultsMultipleMethods`:
  - `OptimizationResultsSingleMethod`: can run several BO loops with the same method with different objective functions and seeds.
  - `OptimizationResultsMultipleMethods`: can run several `OptimizationResultsSingleMethod` instances with different methods.
  
  These classes can only run the BO loops in sequence, not in parallel. For this reason, their full intended usage is now deprecated in favor of `submit.py`, which submits jobs to repeatedly run `single_run.py`. `single_run.py` simply uses `OptimizationResultsSingleMethod` with a single objective function and a single BO seed.

- **Plotting & Utility Functions:**  
  Helper routines for plotting optimization trajectories, generating random GP realizations, and applying outcome transforms. (Much of these are now deprecated in favor of `plot_run.py`.)

## Utility Functions

### Domain-Specific Utilities (`utils/`)
- `utils.py`: Comprehensive suite of helper functions for BO operations including outcome transformations, kernel and model setup, JSON serialization, tensor padding, and data/configuration management.
- `nn_utils.py`: Custom PyTorch modules for BO including dense layers, learnable positive parameters, custom softplus activations, and PointNet layers with various pooling strategies. Includes helper routines for tensor dimension checking and masked softmax operations.
- `plot_utils.py`: Plotting utilities for BO experiments.
- `plot_sorting.py`: Utilities for sorting and organizing plot results.
- `exact_gp_computations.py`: Exact GP posterior calculations.
- `constants.py`: Global constants (e.g., HPOB_DATA_DIR).
- `basic_model_save_utils.py`: Basic model saving utilities.

### General-Purpose Utilities (`utils_general/`)
- `utils.py`: General utility functions.
- `nn_utils.py`: General neural network utilities.
- `plot_utils.py`: General plotting utilities.
- `io_utils.py`: I/O helper functions.
- `math_utils.py`: General mathematical utilities.
- `tictoc.py`: Timing utilities.
- `iterable_utils.py`: Iterable manipulation utilities (moved from `utils/utils.py`).
- `dataset_with_models.py`: Provides a mechanism for creating a hierarchy of classes that represents datasets and can optionally have a GP model attached to each item in the dataset (moved from `datasets/`).
- `saveable_object.py`: Object serialization/persistence.
- `basic_model_save_utils.py`: Basic model saving/loading utilities.
- `torch_module_save_utils.py`: PyTorch module saving/loading utilities.
- `training/`: Training utilities
  - `single_trainer.py`: Single model trainer class.
  - `train_or_test_loop.py`: Training and testing loop implementations.
  - `train_utils.py`: General training utilities.

### Training Utilities for Acquisition Functions (`utils_train/`)
- `acquisition_function_net.py`: Core NN architectures (PointNet, Transformer-based).
- `acquisition_function_net_constants.py`: Constants for acquisition function networks.
- `train_acquisition_function_net.py`: Training logic and model persistence.
- `model_save_utils.py`: Model saving/loading and configuration parsing.
- `train_or_test_loop.py`: Training/testing loops for acquisition functions.
- `train_utils.py`: Training utilities for acquisition functions.



# How to add a new parameter to the experiments
To add a new parameter to the experiments, you should at least add to `config/train_acqf.yml`.
## Adding a new NN training parameter
To add a new NN training parameter to the experiments,
you will need to do the following:
- Add the parameter to the function `get_cmd_options_train_acqf` in `submit_train.py`
- Modify the function `validate_single_train_args` in `utils_train/model_save_utils.py` as appropriate (only if needed) to parse the new parameter.
- Add the parameter to `add_single_train_args_and_return_info` in `utils_train/model_save_utils.py`.
### Adding a NN architecture parameter
If the NN training parameter is specifically a NN architecture parameter, then you will also need to modify `initialize_module_from_args` in `utils_train/model_save_utils.py`.
### Adding a NN training parameter that is not a NN architecture parameter
If the NN training parameter is not a NN architecture parameter, then you will also need to:
- modify `_get_training_config` in `utils_train/model_save_utils.py`.
- modify the function `run_train` in `single_train.py` to pass the new parameter to the `train_acquisition_function_net` function.
## Adding a new dataset type
To add a new dataset type, you will need to do the following:
- Add a new dataset manager in `datasets/` (see `datasets/hpob_acquisition_dataset_manager.py` for an example)
- Add the new dataset type in the required places in `dataset_factory.py`.
- Add to `get_cmd_options_train_acqf` in `submit_train.py` the command-line arguments that are specific to the new dataset type.
- Add the dataset type to `config/train_acqf.yml`
