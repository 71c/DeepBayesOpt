#!/bin/bash
#SBATCH -n 1                          # Total number of cores (tasks) requested (1 by default)
#SBATCH --get-user-env                # retrieve the users login environment
#SBATCH --requeue                     # requeue the job if it fails
#SBATCH --exclude=kuleshov-compute-01   # Exclude this node because it currently has CUDA errors
#SBATCH --exclude=kuleshov-compute-03   # Exclude this node because it currently has CUDA errors
#SBATCH --exclude=bhattacharjee-compute-02   # Exclude this node because it currently has CUDA errors

# Number of times to requeue the job if it fails
MAX_RETRIES=4

# Store Job ID, Array Job ID, and Array Task ID in a string variable
# Version 1: j%j-A%A_a%a
# Version 2: j${SLURM_JOB_ID}-A${SLURM_ARRAY_JOB_ID}_a${SLURM_ARRAY_TASK_ID}
JOB_DESC="j${SLURM_JOB_ID}-A${SLURM_ARRAY_JOB_ID}_a${SLURM_ARRAY_TASK_ID}"

# Obtain these variables from the command line
commands_file=$1
sweep_dir=$2
mail=$3

send_email_notification() {
    local subject="$1"
    local message="$2"
    if [ -n "$mail" ]; then
        # Use the mail command to send an email
        echo "$message" | mail -s "$subject" "$mail"
    fi
}

# Define these varaibles
command=$(sed -n "${SLURM_ARRAY_TASK_ID}p" $commands_file)
attempt_file="${sweep_dir}/attempt-$JOB_DESC.txt"
fails_dir="${sweep_dir}/failed"

# Load the conda environment
eval "$(/share/apps/anaconda3/2022.10/bin/conda shell.bash hook)"
conda activate alon2

# Attempt number of this job
if [ -f "$attempt_file" ]; then
    attempt=$(cat "$attempt_file")
else
    attempt=1
fi

FAIL_NAME_BASE="${fails_dir}/${JOB_DESC}-attempt${attempt}"

out="Executing command: $command
Attempt: $attempt"
err=""

requeue_job () {
    if [ "$attempt" -le "$MAX_RETRIES" ]; then
        err="$err
Job $JOB_DESC failed on attempt $attempt. Requeuing attempt $((attempt + 1)) of $MAX_RETRIES."
        echo "$((attempt + 1))" > "$attempt_file"
        scontrol requeue "$SLURM_JOB_ID"
    else
        err="$err
Job $JOB_DESC failed. Giving up after $MAX_RETRIES attempts."
        rm -f "$attempt_file"
    fi
}

log_failure() {
    mkdir -p "$fails_dir"
    echo "$err" > "${fail_name}.err"
    echo "$out" > "${fail_name}.out"
    local email_subject="Job ${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID} failed with exit code $exit_code on attempt $attempt. $1"
    local email_message="Please check ${fail_name}.err and ${fail_name}.out for details:

out:
$out

err:
$err"
    send_email_notification "$email_subject" "$email_message"
}

handle_gpu_error_and_exit() {
    local error_message="$1"
    local log_message="$2"
    
    err="$err
$error_message"
    requeue_job
    fail_name="${FAIL_NAME_BASE}-exit${exit_code}"
    log_failure "$log_message"
    echo "$out"
    echo "$err" >&2
    exit $exit_code
}

# Debugging CUDA availability
# store the output of nvidia-smi, or the empty string if there is no gpu
# ("2> /dev/null" makes the stderr not outputted)
nvidia_smi_output=$(nvidia-smi 2> /dev/null)




# -n tells whether a variable is nonempty (-z is the opposite, tells whether it's empty)
if [[ -n "$nvidia_smi_output" && "$nvidia_smi_output" != "No devices were found" ]]; then


out="$out
Host: $(hostname)
Partition: $SLURM_JOB_PARTITION
GPU devices (CUDA_VISIBLE_DEVICES): $CUDA_VISIBLE_DEVICES
nvidia-smi output:
$nvidia_smi_output"

# The following variable is the empty string iff CUDA is both available AND usable.
# Otherwise, it either prints "CUDA is not available!" or raises an error.
unavail=$(python -c "
import torch
if torch.cuda.is_available():
    torch.tensor([0.0], device='cuda')
else:
    print('CUDA not available')
")

if [[ -n "$unavail" || "$nvidia_smi_output" =~ 'ERR' ]]; then
    python -c "import torch; print(f'Torch CUDA version: {torch.version.cuda}')"
    out="$out
$unavail"
    exit_code=1
    handle_gpu_error_and_exit "$unavail" "CUDA not available, Requeuing job"
fi

fi

echo "$out"
echo "$err" >&2

# Run the command, capturing stdout, stderr, and exit code
out_file=$(mktemp)
err_file=$(mktemp)
bash -c "$command" > >(tee "$out_file") 2> >(tee "$err_file" >&2)
exit_code=$?
cmd_out=$(<"$out_file")
cmd_err=$(<"$err_file")
rm "$out_file" "$err_file"

out="$out
$cmd_out"
err="$err
$cmd_err"

# Check if the command was successful
if [ "$exit_code" -eq 0 ]; then
    echo "Command succeeded"
    rm -f "$attempt_file"
else
    new_err="

[Command failed with exit code $exit_code.]"

    fail_name="${FAIL_NAME_BASE}-exit${exit_code}"

    # Check for GPU-related errors that should be requeued even with exit code 1
    gpu_error_patterns=(
        "CUDA error: no kernel image is available for execution on the device"
        "CUDA capability sm_[0-9]+ is not compatible with the current PyTorch installation"
        "NVIDIA .* with CUDA capability sm_[0-9]+ is not compatible with the current PyTorch installation"
        "RuntimeError: CUDA error:"
        "CUDA out of memory"
        "RuntimeError: Attempting to deserialize object on a CUDA device but"
    )
    
    is_gpu_error=false
    for pattern in "${gpu_error_patterns[@]}"; do
        if echo "$cmd_err" | grep -qE "$pattern"; then
            is_gpu_error=true
            break
        fi
    done

    is_exit_code_1_or_2=false
    if [ "$exit_code" -eq 1 ] || [ "$exit_code" -eq 2 ]; then
        is_exit_code_1_or_2=true
    fi
    
    if [ "$is_exit_code_1_or_2" = true ] && [ "$is_gpu_error" = true ]; then
        # Handle GPU errors the same way as CUDA unavailable errors
        handle_gpu_error_and_exit "$new_err
GPU-related runtime error detected. Requeuing job." "GPU runtime error, Requeuing job"
    elif [ "$is_exit_code_1_or_2" = true ]; then
        new_err="$new_err
This is likely a user error. Not requeuing.
Please check ${fail_name}.err and ${fail_name}.out for details.
Canceling the job array ${SLURM_ARRAY_JOB_ID}_*."
        rm -f "$attempt_file"
        subject="Canceling job array"
    else
        requeue_job
        subject="Requeuing job"
    fi
    echo "$new_err" >&2
    err="$err
$new_err"
    log_failure "$subject"

    # It seems that I must cancel the job array AFTER the email notification is sent (?)
    # because previously, the email notification was not sent.
    if [ "$exit_code" -eq 1 ] && [ "$is_gpu_error" = false ]; then
        scancel "${SLURM_ARRAY_JOB_ID}"
    fi
fi

exit $exit_code
