#!/bin/bash
#SBATCH -n 1                          # Total number of cores (tasks) requested (1 by default)
#SBATCH --get-user-env                # retrieve the users login environment
#SBATCH --requeue                     # requeue the job if it fails
#SBATCH --exclude=jjs533-compute-01   # Exclude this node from the job because it got cuda problems with the H100

# Number of times to requeue the job if it fails
MAX_RETRIES=3

# Store Job ID, Array Job ID, and Array Task ID in a string variable
# Version 1: j%j-A%A_a%a
# Version 2: j${SLURM_JOB_ID}-A${SLURM_ARRAY_JOB_ID}_a${SLURM_ARRAY_TASK_ID}
JOB_DESC="j${SLURM_JOB_ID}-A${SLURM_ARRAY_JOB_ID}_a${SLURM_ARRAY_TASK_ID}"

# Obtain these variables from the command line
commands_file=$1
sweep_dir=$2

# Define these varaibles
command=$(sed -n "${SLURM_ARRAY_TASK_ID}p" $commands_file)
attempt_file="${sweep_dir}/attempt-$JOB_DESC.txt"

# Load the conda environment
eval "$(/share/apps/anaconda3/2022.10/bin/conda shell.bash hook)"
conda activate alon2

# Attempt number of this job
if [ -f "$attempt_file" ]; then
    attempt=$(cat "$attempt_file")
else
    attempt=1
fi

echo "Executing command: $command"
echo "Attempt: $attempt"
echo ""

# This function can optionally be called when the job fails
requeue_job_and_exit () {
    if [ "$attempt" -le "$MAX_RETRIES" ]; then
        echo "Requeuing..."
        echo "$((attempt + 1))" > "$attempt_file"
        scontrol requeue "$SLURM_JOB_ID"
    else
        echo "Max requeue attempts (${MAX_RETRIES}) reached, giving up."
        rm -f "$attempt_file"
    fi
    exit 1
}

# Debugging CUDA availability
# store the output of nvidia-smi, or the empty string if there is no gpu
# ("2> /dev/null" makes the stderr not outputted)
nvidia_smi_output=$(nvidia-smi 2> /dev/null)
# -n tells whether a variable is nonempty (-z is the opposite, tells whether it's empty)
if [ -n "$nvidia_smi_output" ]; then
    echo "Host: $(hostname)"
    echo "Partition: $SLURM_JOB_PARTITION"
    echo "GPU devices (CUDA_VISIBLE_DEVICES): $CUDA_VISIBLE_DEVICES"
    echo "nvidia-smi output:"
    echo "$nvidia_smi_output"

    unavail=$(python -c "import torch;print('' if torch.cuda.is_available() else 'CUDA is not available!')")
    if [ -n "$unavail" ]; then
        echo "$unavail"
        python -c "import torch; print(f'Torch CUDA version: {torch.version.cuda}')"
        requeue_job_and_exit
    fi
fi

# Run the command, capturing stdout, stderr, and exit code
out_file=$(mktemp)
err_file=$(mktemp)
bash -c "$command" > "$out_file" 2> "$err_file"
exit_code=$?
out=$(<"$out_file")
err=$(<"$err_file")
rm "$out_file" "$err_file"

# Print stdout back to stdout and stderr back to stderr
echo "$out"
echo "$err" >&2

# Check if the command was successful
if [ "$exit_code" -ne 0 ]; then
    # If the command failed, log the error
    echo "" >&2
    echo "[Command failed with exit code $exit_code.]" >&2
    fails_dir="${sweep_dir}/failed"
    mkdir -p "$fails_dir"
    echo "$err" > "${fails_dir}/${JOB_DESC}-attempt${attempt}-exit${exit_code}.err"
else
    echo "Command succeeded"
fi

rm -f "$attempt_file"
exit $exit_code
