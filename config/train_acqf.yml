parameters:
  function_samples_dataset:
    parameters:
      gp:
        parameters:
          dimension:
            values: [8, 16, 32]
          kernel:
            values: ["Matern52"]
          lengthscale:
            values: [0.1]
          outcome_transform:
            values:
            - value: null
            - value: exp
              parameters:
                sigma:
                  value: 0.8
          randomize_params:
            values: [false, true]
      standardize_outcomes:
        value: false
      train_samples_size:
        values: [1000, 10000, 100000]
      test_samples_size:
        value: 10000
  acquisition_dataset:
    parameters:
      train_acquisition_size:
        value: 30000
      replacement:
        values: [true, false]
      test_expansion_factor:
        value: 1
      n_candidates:
        values: [1, 5, 25]
      min_history:
        value: 1
      max_history:
        value: 100
      samples_addition_amount:
        value: 5
  architecture:
    parameters:
      layer_width:
        values: [100, 250, 500]
        value: 250
      dropout:
        value: null
      standardize_nn_history_outcomes:
        value: false
      architecture: # TODO: Will change/improve this in the future
        values:
        - value: pointnet
        - value: transformer
          parameters:
            num_heads:
              value: 4
            num_layers:
              value: 2
  training:
    parameters:
      method:
        values:
        - value: gittins
          parameters:
            gi_loss_normalization:
              values: ["normal", null]
            lamda_config:
              parameters:
              - lamda_min:
                  value: 1.e-4
                lamda_max:
                  value: 1.e+0
              - lamda:
                  values: [1.e-5, 1.e-2]
        - value: mse_ei
        - value: policy_gradient
          parameters:
            include_alpha:
              values:
              - value: true
                parameters:
                  learn_alpha:
                    values: [true, false]
                  initial_alpha:
                    value: 1.0
                  alpha_increment:
                    value: null
              - value: false
      learning_rate:
        values: [3.e-3, 3.e-4]
      weight_decay:
        value: null
      batch_size:
        values: [32, 128]
      epochs:
        value: 500
      use_maxei:
        values: [false, true]
      early_stopping:
        values:
        - value: true
          parameters:
            patience:
              value: 30
            min_delta:
              value: 0.0
            cumulative_delta:
              value: false
        - value: false
      lr_scheduler:
        values:
        - value: null
        - value: ReduceLROnPlateau
          parameters:
            lr_scheduler_patience:
              value: 15
            lr_scheduler_factor:
              value: 0.1
            lr_scheduler_min_lr:
              value: 0.0
            lr_scheduler_cooldown:
              value: 0
