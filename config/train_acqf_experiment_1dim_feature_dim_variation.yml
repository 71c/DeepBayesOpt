parameters:
  max_history:
    value: 20
  dimension:
    value: 1
  lengthscale:
    value: 0.05
  method:
    values: ['gittins']
  gi_loss_normalization:
    value: normal
  lamda:
    value: 1.00e-02
  train_samples_size: # TODO: change to a single value
    values: [40, 320, 2560]
  samples_addition_amount: # TODO: change to a single value
    values: [5, 30, 180]
  test_samples_size:
    value: 1000
  test_expansion_factor:
    value: 100
  n_candidates:
    value: 1
  layer_width:
    values: [8, 32]
  encoded_history_dim:
    values: [2, 8, 32, 128]
  architecture:
    value: pointnet
  include_best_y:
    values: [false, true]
  x_cand_input:
    values:
    - "subtract-lossy"
  epochs:
    value: 4000
  learning_rate:
    values: [5.20e-05, 3.00e-04, 1.73e-03]
  batch_size:
    value: 512
  dropout:
    values: [null]
  weight_decay:
    values: [null]
  lr_scheduler:
    values: [null]
  lr_scheduler_patience:
    value: 300
  lr_scheduler_factor:
    value: 0.5
  early_stopping:
    value: false
  patience:
    value: 1000
  min_delta:
    value: 1.00e-09
