{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0921fae7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    }
   ],
   "source": [
    "from acquisition_function_net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eba8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_body = AcquisitionFunctionBodyPointnetV1and2(\n",
    "    dimension=6, n_hist_out=1, n_acqf_params=0,\n",
    "    \n",
    "    history_enc_hidden_dims=[100, 100],\n",
    "    pooling=\"max\",\n",
    "    encoded_history_dim=100,\n",
    "\n",
    "    input_xcand_to_local_nn=True,\n",
    "    input_xcand_to_final_mlp=False,\n",
    "    \n",
    "    activation_at_end_pointnet=True,\n",
    "    layer_norm_pointnet=False,\n",
    "    dropout_pointnet=None,\n",
    "    activation_pointnet=\"relu\",\n",
    "\n",
    "    include_best_y=False,\n",
    "    n_pointnets=1\n",
    ")\n",
    "model_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad122ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoPartAcquisitionFunctionNetFixedHistoryOutputDim(\n",
      "  (af_body): AcquisitionFunctionBodyPointnetV1and2(\n",
      "    (pointnet): PointNetLayer(\n",
      "      (network): Dense(\n",
      "        (0): Linear(in_features=17, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (af_head): AcquisitionFunctionNetFinalMLPSoftmaxExponentiate(\n",
      "    (dense): Dense(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=4, bias=True)\n",
      "    )\n",
      "    (transform): SoftmaxOrSoftplusLayer(\n",
      "      (softplus): Softplus(beta=1.0, threshold=20)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2038, 0.6025, 1.1998, 1.5677],\n",
       "         [0.2973, 0.6669, 1.3373, 1.5000]],\n",
       "\n",
       "        [[0.2254, 0.6487, 1.4972, 1.8132],\n",
       "         [0.3017, 0.8433, 1.6667, 1.5163]],\n",
       "\n",
       "        [[0.2062, 0.6810, 1.2439, 1.5903],\n",
       "         [0.1585, 0.7847, 1.6275, 1.8296]],\n",
       "\n",
       "        [[0.2769, 0.5962, 1.3208, 1.3376],\n",
       "         [0.2787, 0.7981, 1.3617, 1.2185]],\n",
       "\n",
       "        [[0.3030, 0.9892, 1.5803, 1.3891],\n",
       "         [0.2133, 0.6847, 1.4985, 1.8892]],\n",
       "\n",
       "        [[0.1719, 0.4849, 1.0633, 1.5968],\n",
       "         [0.2634, 0.7327, 1.5219, 1.5877]],\n",
       "\n",
       "        [[0.4226, 0.9686, 1.4642, 1.3840],\n",
       "         [0.4266, 0.7479, 1.3515, 1.4704]],\n",
       "\n",
       "        [[0.2633, 0.7499, 1.4845, 1.5628],\n",
       "         [0.3249, 0.5934, 1.2837, 1.5457]],\n",
       "\n",
       "        [[0.3124, 0.6943, 1.5168, 1.5647],\n",
       "         [0.2528, 0.5837, 1.2725, 1.6169]],\n",
       "\n",
       "        [[0.3476, 0.9063, 1.5148, 1.2070],\n",
       "         [0.2386, 0.7481, 1.7169, 1.5389]]], grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 6\n",
    "n_hist_out = 2\n",
    "n_acqf_params = 3\n",
    "af_body_init_params = dict(\n",
    "    dimension=dimension,\n",
    "    \n",
    "    history_enc_hidden_dims=[100, 100],\n",
    "    pooling=\"max\",\n",
    "    encoded_history_dim=100,\n",
    "\n",
    "    input_xcand_to_local_nn=True,\n",
    "    input_xcand_to_final_mlp=False,\n",
    "    \n",
    "    activation_at_end_pointnet=True,\n",
    "    layer_norm_pointnet=False,\n",
    "    dropout_pointnet=None,\n",
    "    activation_pointnet=\"relu\",\n",
    "\n",
    "    include_best_y=False,\n",
    "    n_pointnets=1)\n",
    "\n",
    "# af_head_class = AcquisitionFunctionNetFinalMLP\n",
    "# af_head_init_params = dict(\n",
    "#     hidden_dims=[100, 100],\n",
    "#     activation=\"relu\",\n",
    "#     layer_norm_before_end=False,\n",
    "#     layer_norm_at_end=False,\n",
    "#     dropout=None\n",
    "# )\n",
    "\n",
    "af_head_class = AcquisitionFunctionNetFinalMLPSoftmaxExponentiate\n",
    "af_head_init_params = dict(\n",
    "    hidden_dims=[100, 100],\n",
    "    activation=\"relu\",\n",
    "    layer_norm_before_end=False,\n",
    "    layer_norm_at_end=False,\n",
    "    dropout=None,\n",
    "\n",
    "    include_alpha=False,\n",
    "    learn_alpha=False,\n",
    "    initial_alpha=1.0,\n",
    "    initial_beta=1.0,\n",
    "    learn_beta=False,\n",
    "    softplus_batchnorm=False,\n",
    "    softplus_batchnorm_momentum=0.1,\n",
    "    positive_linear_at_end=False,\n",
    "    gp_ei_computation=False\n",
    ")\n",
    "\n",
    "af = TwoPartAcquisitionFunctionNetFixedHistoryOutputDim(\n",
    "    output_dim=4,\n",
    "    n_acqf_params=n_acqf_params,\n",
    "    n_hist_out=n_hist_out,\n",
    "    af_body_class=AcquisitionFunctionBodyPointnetV1and2,\n",
    "    af_head_class=af_head_class,\n",
    "    af_body_init_params=af_body_init_params,\n",
    "    af_head_init_params=af_head_init_params,\n",
    "    standardize_outcomes=False\n",
    ")\n",
    "print(af)\n",
    "\n",
    "batch_size = 10\n",
    "n_hist = 5\n",
    "n_cand = 2\n",
    "x_hist = torch.rand(batch_size, n_hist, dimension)\n",
    "y_hist = torch.rand(batch_size, n_hist, n_hist_out)\n",
    "x_cand = torch.rand(batch_size, n_cand, dimension)\n",
    "acqf_params = torch.rand(batch_size, n_cand, n_acqf_params)\n",
    "output = af(x_hist, y_hist, x_cand, acqf_params,\n",
    "            exponentiate=True, softmax=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bc6ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GittinsAcquisitionFunctionNet(\n",
      "  (base_model): TwoPartAcquisitionFunctionNetFixedHistoryOutputDim(\n",
      "    (af_body): AcquisitionFunctionBodyPointnetV1and2(\n",
      "      (pointnet): PointNetLayer(\n",
      "        (network): Dense(\n",
      "          (0): Linear(in_features=15, out_features=100, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "          (5): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (af_head): AcquisitionFunctionNetFinalMLP(\n",
      "      (dense): Dense(\n",
      "        (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4667],\n",
       "         [ 0.3615]],\n",
       "\n",
       "        [[ 0.1217],\n",
       "         [ 0.3055]],\n",
       "\n",
       "        [[ 0.3452],\n",
       "         [ 0.4369]],\n",
       "\n",
       "        [[ 0.2177],\n",
       "         [ 0.0614]],\n",
       "\n",
       "        [[ 0.1412],\n",
       "         [ 0.3216]],\n",
       "\n",
       "        [[ 0.0997],\n",
       "         [ 0.4380]],\n",
       "\n",
       "        [[ 0.0273],\n",
       "         [-0.3770]],\n",
       "\n",
       "        [[ 0.2848],\n",
       "         [ 0.0726]],\n",
       "\n",
       "        [[ 0.3190],\n",
       "         [-0.6046]],\n",
       "\n",
       "        [[ 0.5430],\n",
       "         [ 0.2411]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 6\n",
    "n_acqf_params = 3\n",
    "af_body_init_params = dict(\n",
    "    dimension=dimension,\n",
    "    \n",
    "    history_enc_hidden_dims=[100, 100],\n",
    "    pooling=\"max\",\n",
    "    encoded_history_dim=100,\n",
    "\n",
    "    input_xcand_to_local_nn=True,\n",
    "    input_xcand_to_final_mlp=False,\n",
    "    \n",
    "    activation_at_end_pointnet=True,\n",
    "    layer_norm_pointnet=False,\n",
    "    dropout_pointnet=None,\n",
    "    activation_pointnet=\"relu\",\n",
    "\n",
    "    include_best_y=False,\n",
    "    n_pointnets=1)\n",
    "af_head_init_params = dict(\n",
    "    hidden_dims=[100, 100],\n",
    "    activation=\"relu\",\n",
    "    layer_norm_before_end=False,\n",
    "    layer_norm_at_end=False,\n",
    "    dropout=None\n",
    ")\n",
    "gittins_af = GittinsAcquisitionFunctionNet(\n",
    "    af_class=TwoPartAcquisitionFunctionNetFixedHistoryOutputDim,\n",
    "    variable_lambda=True,\n",
    "    costs_in_history=True,\n",
    "    cost_is_input=False,\n",
    "    af_body_class=AcquisitionFunctionBodyPointnetV1and2,\n",
    "    af_head_class=AcquisitionFunctionNetFinalMLP,\n",
    "    af_body_init_params=af_body_init_params,\n",
    "    af_head_init_params=af_head_init_params,\n",
    "    standardize_outcomes=False\n",
    ")\n",
    "print(gittins_af)\n",
    "\n",
    "gittins_af.save_init('test_ddd')\n",
    "\n",
    "batch_size = 10\n",
    "n_hist = 5\n",
    "n_cand = 2\n",
    "x_hist = torch.rand(batch_size, n_hist, dimension)\n",
    "y_hist = torch.rand(batch_size, n_hist, 1)\n",
    "log_cost_hist = torch.rand(batch_size, n_hist, 1)\n",
    "x_cand = torch.rand(batch_size, n_cand, dimension)\n",
    "log_lambda = torch.rand(batch_size, n_cand, 1)\n",
    "output = gittins_af(x_hist, y_hist, x_cand,\n",
    "                    lambda_cand=log_lambda,\n",
    "                    cost_hist=log_cost_hist,\n",
    "                    is_log=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e226ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.get_info_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dbce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.save_init(\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SaveableObject\n",
    "SaveableObject.load_init(\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f613f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Dummy(nn.Module):\n",
    "    def __init__(self, the_module):\n",
    "        super().__init__()\n",
    "        self.the_module = the_module\n",
    "        self.register_forward_pre_hook(self._my_pre_forward_hook)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _my_pre_forward_hook(module, inputs):\n",
    "        # do something\n",
    "        return inputs_modified\n",
    "    \n",
    "    def forward(self, a, b, **kwargs):\n",
    "        return self.the_module(a, b, **kwargs)\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self._dummy = _Dummy()\n",
    "\n",
    "    def forward(self, a, b=3, **kwargs):\n",
    "        return self._dummy(a, b, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a': 1, 'b': 2}\n",
    "for k, v in d.items():\n",
    "    if v == 1:\n",
    "        d[k] = v + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_utils import SaveableObject\n",
    "SaveableObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Ta(nn.Module):\n",
    "    pass\n",
    "b = Ta()\n",
    "b(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369065e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2.0\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m ei_helper_inverse(x)\n\u001b[0;32m----> 5\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from utils import ei_helper_inverse\n",
    "import torch\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = ei_helper_inverse(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d45f19a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5743, 0.1013])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import gi_normal\n",
    "import torch\n",
    "\n",
    "c = torch.tensor([0.00001, 0.1])\n",
    "mu = torch.tensor([-0.3, 0.2])\n",
    "sigma = torch.tensor([1.7, 0.06])\n",
    "gi_normal(c, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "456682a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.batch_shape=torch.Size([])\n",
      "posterior.mean.shape=torch.Size([7, 2]), posterior.variance.shape=torch.Size([7, 2])\n",
      "posterior.mean[..., 0].shape=torch.Size([7])\n",
      "posterior.mean.squeeze(-1).shape=torch.Size([7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aloja/opt/anaconda3/lib/python3.9/site-packages/botorch/models/utils/assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([-1.3147, -1.7337]), std = tensor([2.3553, 0.9760])). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<botorch.posteriors.transformed.TransformedPosterior at 0x7f8c985d40a0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Log\n",
    "\n",
    "batch = 4\n",
    "n_hist = 5\n",
    "d = 6\n",
    "m = 2\n",
    "model = SingleTaskGP(\n",
    "    train_X=torch.rand(n_hist, d),\n",
    "    train_Y=torch.rand(n_hist, m),\n",
    "    outcome_transform=Log()\n",
    ")\n",
    "print(f\"{model.batch_shape=}\")\n",
    "\n",
    "posterior = model.posterior(torch.rand(7, d))\n",
    "print(f\"{posterior.mean.shape=}, {posterior.variance.shape=}\")\n",
    "print(f\"{posterior.mean[..., 0].shape=}\")\n",
    "print(f\"{posterior.mean.squeeze(-1).shape=}\")\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59dfc51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand([6,3]).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58fedaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'method': 'gittins', 'normalize_gi_loss': True},\n",
       " {'method': 'gittins', 'normalize_gi_loss': False},\n",
       " {'method': 'another_method', 'a': 'value1', 't': 1},\n",
       " {'method': 'another_method', 'a': 'value1', 't': 2},\n",
       " {'method': 'another_method', 'a': 'value1', 't': 3},\n",
       " {'method': 'another_method', 'a': 'value2', 't': 1},\n",
       " {'method': 'another_method', 'a': 'value2', 't': 2},\n",
       " {'method': 'another_method', 'a': 'value2', 't': 3}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define your parameter dependencies\n",
    "parameter_dependencies = {\n",
    "    \"gittins\": {\n",
    "        \"normalize_gi_loss\": [True, False],  # Only relevant for gittins\n",
    "    },\n",
    "    \"another_method\": {\n",
    "        \"a\": [\"value1\", \"value2\"],\n",
    "        \"t\": [1, 2, 3] # Relevant only for another_method\n",
    "    },\n",
    "    # Add more methods and their specific parameters as needed\n",
    "}\n",
    "\n",
    "# Generate the Cartesian product for each method\n",
    "sweep_parameters = []\n",
    "for method, params in parameter_dependencies.items():\n",
    "    param_names, param_values = zip(*params.items())  # Extract names and values\n",
    "    product = itertools.product(*param_values)  # Cartesian product of relevant values\n",
    "    for combination in product:\n",
    "        config = {\"method\": method}  # Base method\n",
    "        config.update(dict(zip(param_names, combination)))  # Add dependent params\n",
    "        sweep_parameters.append(config)\n",
    "\n",
    "sweep_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c6095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in C: A.__new__((32,), {'a': 4}) called\n",
      "A.__init__(b=32, {'a': 4}) called\n",
      "in C: A.__new__((), {'b': 32, 'a': 4}) called\n",
      "A.__init__(b=32, {'a': 4}) called\n"
     ]
    }
   ],
   "source": [
    "from utils import SaveableObject\n",
    "\n",
    "class C:\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        print(f\"in C: {cls.__name__}.__new__({args}, {kwargs}) called\")\n",
    "        return super().__new__(cls)\n",
    "    def __init__(self, b, **kwargs):\n",
    "        print(f\"{self.__class__.__name__}.__init__({b=}, {kwargs}) called\")\n",
    "    pass\n",
    "\n",
    "class A(C, SaveableObject):\n",
    "    # def __init__(self, b=4, **kwargs):\n",
    "    #     print(f\"{self.__class__.__name__}.__init__({b=}, {kwargs}) called\")\n",
    "    pass\n",
    "\n",
    "a = A(32, a=4)\n",
    "a.save_init(\"test\")\n",
    "b = A.load_init(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95222f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Trying to load a W instance but the JSON file contains a A instance. Instead of calling W.load_init, call SaveableObject.load_init  or A.load_init",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mW\u001b[39;00m(SaveableObject):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/cornell/RESEARCH/BayesOpt/CODE/DeepBayesOpt/utils.py:1594\u001b[0m, in \u001b[0;36mSaveableObject.load_init\u001b[0;34m(cls, folder)\u001b[0m\n\u001b[1;32m   1592\u001b[0m typ, kwargs \u001b[38;5;241m=\u001b[39m _info_dict_to_instance(info_dict)\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SaveableObject \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m typ:\n\u001b[0;32m-> 1594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instance but the JSON file \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1596\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontains a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instance. Instead of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1597\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.load_init, call SaveableObject.load_init \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1598\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.load_init\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1599\u001b[0m     )\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typ(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Trying to load a W instance but the JSON file contains a A instance. Instead of calling W.load_init, call SaveableObject.load_init  or A.load_init"
     ]
    }
   ],
   "source": [
    "class W(SaveableObject):\n",
    "    pass\n",
    "\n",
    "W.load_init(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6aa131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in C: A.__new__((), {'b': 32, 'a': 4}) called\n",
      "A.__init__(b=32, {'a': 4}) called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.A at 0x7f951a7afee0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SaveableObject.load_init(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090aa555",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class SaveableObject with abstract method __init__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m w\u001b[38;5;241m=\u001b[39m\u001b[43mSaveableObject\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class SaveableObject with abstract method __init__"
     ]
    }
   ],
   "source": [
    "w=SaveableObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed31b748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__init__': <function __main__.A.__init__(self, b=4, **kwargs)>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa33405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              '__new__': <staticmethod at 0x7f77e981a880>,\n",
       "              '__dict__': <attribute '__dict__' of 'C' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'C' objects>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a64ee5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function object.__new__(*args, **kwargs)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class B: pass\n",
    "B.__new__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce1c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
