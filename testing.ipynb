{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0921fae7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    }
   ],
   "source": [
    "from acquisition_function_net import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eba8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_body = AcquisitionFunctionBodyPointnetV1and2(\n",
    "    dimension=6, n_hist_out=1, n_acqf_params=0,\n",
    "    \n",
    "    history_enc_hidden_dims=[100, 100],\n",
    "    pooling=\"max\",\n",
    "    encoded_history_dim=100,\n",
    "\n",
    "    input_xcand_to_local_nn=True,\n",
    "    input_xcand_to_final_mlp=False,\n",
    "    \n",
    "    activation_at_end_pointnet=True,\n",
    "    layer_norm_pointnet=False,\n",
    "    dropout_pointnet=None,\n",
    "    activation_pointnet=\"relu\",\n",
    "\n",
    "    include_best_y=False,\n",
    "    n_pointnets=1\n",
    ")\n",
    "model_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad122ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoPartAcquisitionFunctionNetFixedHistoryOutputDim(\n",
      "  (af_body): AcquisitionFunctionBodyPointnetV1and2(\n",
      "    (pointnet): PointNetLayer(\n",
      "      (network): Dense(\n",
      "        (0): Linear(in_features=17, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (5): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (af_head): AcquisitionFunctionNetFinalMLPSoftmaxExponentiate(\n",
      "    (dense): Dense(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=4, bias=True)\n",
      "    )\n",
      "    (transform): SoftmaxOrSoftplusLayer(\n",
      "      (softplus): Softplus(beta=1.0, threshold=20)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2038, 0.6025, 1.1998, 1.5677],\n",
       "         [0.2973, 0.6669, 1.3373, 1.5000]],\n",
       "\n",
       "        [[0.2254, 0.6487, 1.4972, 1.8132],\n",
       "         [0.3017, 0.8433, 1.6667, 1.5163]],\n",
       "\n",
       "        [[0.2062, 0.6810, 1.2439, 1.5903],\n",
       "         [0.1585, 0.7847, 1.6275, 1.8296]],\n",
       "\n",
       "        [[0.2769, 0.5962, 1.3208, 1.3376],\n",
       "         [0.2787, 0.7981, 1.3617, 1.2185]],\n",
       "\n",
       "        [[0.3030, 0.9892, 1.5803, 1.3891],\n",
       "         [0.2133, 0.6847, 1.4985, 1.8892]],\n",
       "\n",
       "        [[0.1719, 0.4849, 1.0633, 1.5968],\n",
       "         [0.2634, 0.7327, 1.5219, 1.5877]],\n",
       "\n",
       "        [[0.4226, 0.9686, 1.4642, 1.3840],\n",
       "         [0.4266, 0.7479, 1.3515, 1.4704]],\n",
       "\n",
       "        [[0.2633, 0.7499, 1.4845, 1.5628],\n",
       "         [0.3249, 0.5934, 1.2837, 1.5457]],\n",
       "\n",
       "        [[0.3124, 0.6943, 1.5168, 1.5647],\n",
       "         [0.2528, 0.5837, 1.2725, 1.6169]],\n",
       "\n",
       "        [[0.3476, 0.9063, 1.5148, 1.2070],\n",
       "         [0.2386, 0.7481, 1.7169, 1.5389]]], grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 6\n",
    "n_hist_out = 2\n",
    "n_acqf_params = 3\n",
    "af_body_init_params = dict(\n",
    "    dimension=dimension,\n",
    "    \n",
    "    history_enc_hidden_dims=[100, 100],\n",
    "    pooling=\"max\",\n",
    "    encoded_history_dim=100,\n",
    "\n",
    "    input_xcand_to_local_nn=True,\n",
    "    input_xcand_to_final_mlp=False,\n",
    "    \n",
    "    activation_at_end_pointnet=True,\n",
    "    layer_norm_pointnet=False,\n",
    "    dropout_pointnet=None,\n",
    "    activation_pointnet=\"relu\",\n",
    "\n",
    "    include_best_y=False,\n",
    "    n_pointnets=1)\n",
    "\n",
    "# af_head_class = AcquisitionFunctionNetFinalMLP\n",
    "# af_head_init_params = dict(\n",
    "#     hidden_dims=[100, 100],\n",
    "#     activation=\"relu\",\n",
    "#     layer_norm_before_end=False,\n",
    "#     layer_norm_at_end=False,\n",
    "#     dropout=None\n",
    "# )\n",
    "\n",
    "af_head_class = AcquisitionFunctionNetFinalMLPSoftmaxExponentiate\n",
    "af_head_init_params = dict(\n",
    "    hidden_dims=[100, 100],\n",
    "    activation=\"relu\",\n",
    "    layer_norm_before_end=False,\n",
    "    layer_norm_at_end=False,\n",
    "    dropout=None,\n",
    "\n",
    "    include_alpha=False,\n",
    "    learn_alpha=False,\n",
    "    initial_alpha=1.0,\n",
    "    initial_beta=1.0,\n",
    "    learn_beta=False,\n",
    "    softplus_batchnorm=False,\n",
    "    softplus_batchnorm_momentum=0.1,\n",
    "    positive_linear_at_end=False,\n",
    "    gp_ei_computation=False\n",
    ")\n",
    "\n",
    "af = TwoPartAcquisitionFunctionNetFixedHistoryOutputDim(\n",
    "    output_dim=4,\n",
    "    n_acqf_params=n_acqf_params,\n",
    "    n_hist_out=n_hist_out,\n",
    "    af_body_class=AcquisitionFunctionBodyPointnetV1and2,\n",
    "    af_head_class=af_head_class,\n",
    "    af_body_init_params=af_body_init_params,\n",
    "    af_head_init_params=af_head_init_params,\n",
    "    standardize_outcomes=False\n",
    ")\n",
    "print(af)\n",
    "\n",
    "batch_size = 10\n",
    "n_hist = 5\n",
    "n_cand = 2\n",
    "x_hist = torch.rand(batch_size, n_hist, dimension)\n",
    "y_hist = torch.rand(batch_size, n_hist, n_hist_out)\n",
    "x_cand = torch.rand(batch_size, n_cand, dimension)\n",
    "acqf_params = torch.rand(batch_size, n_cand, n_acqf_params)\n",
    "output = af(x_hist, y_hist, x_cand, acqf_params,\n",
    "            exponentiate=True, softmax=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bc6ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GittinsAcquisitionFunctionNet(\n",
      "  (base_model): TwoPartAcquisitionFunctionNetFixedHistoryOutputDim(\n",
      "    (af_body): AcquisitionFunctionBodyPointnetV1and2(\n",
      "      (pointnet): PointNetLayer(\n",
      "        (network): Dense(\n",
      "          (0): Linear(in_features=15, out_features=100, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "          (5): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (af_head): AcquisitionFunctionNetFinalMLP(\n",
      "      (dense): Dense(\n",
      "        (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=100, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4667],\n",
       "         [ 0.3615]],\n",
       "\n",
       "        [[ 0.1217],\n",
       "         [ 0.3055]],\n",
       "\n",
       "        [[ 0.3452],\n",
       "         [ 0.4369]],\n",
       "\n",
       "        [[ 0.2177],\n",
       "         [ 0.0614]],\n",
       "\n",
       "        [[ 0.1412],\n",
       "         [ 0.3216]],\n",
       "\n",
       "        [[ 0.0997],\n",
       "         [ 0.4380]],\n",
       "\n",
       "        [[ 0.0273],\n",
       "         [-0.3770]],\n",
       "\n",
       "        [[ 0.2848],\n",
       "         [ 0.0726]],\n",
       "\n",
       "        [[ 0.3190],\n",
       "         [-0.6046]],\n",
       "\n",
       "        [[ 0.5430],\n",
       "         [ 0.2411]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 6\n",
    "n_acqf_params = 3\n",
    "af_body_init_params = dict(\n",
    "    dimension=dimension,\n",
    "    \n",
    "    history_enc_hidden_dims=[100, 100],\n",
    "    pooling=\"max\",\n",
    "    encoded_history_dim=100,\n",
    "\n",
    "    input_xcand_to_local_nn=True,\n",
    "    input_xcand_to_final_mlp=False,\n",
    "    \n",
    "    activation_at_end_pointnet=True,\n",
    "    layer_norm_pointnet=False,\n",
    "    dropout_pointnet=None,\n",
    "    activation_pointnet=\"relu\",\n",
    "\n",
    "    include_best_y=False,\n",
    "    n_pointnets=1)\n",
    "af_head_init_params = dict(\n",
    "    hidden_dims=[100, 100],\n",
    "    activation=\"relu\",\n",
    "    layer_norm_before_end=False,\n",
    "    layer_norm_at_end=False,\n",
    "    dropout=None\n",
    ")\n",
    "gittins_af = GittinsAcquisitionFunctionNet(\n",
    "    af_class=TwoPartAcquisitionFunctionNetFixedHistoryOutputDim,\n",
    "    variable_lambda=True,\n",
    "    costs_in_history=True,\n",
    "    cost_is_input=False,\n",
    "    af_body_class=AcquisitionFunctionBodyPointnetV1and2,\n",
    "    af_head_class=AcquisitionFunctionNetFinalMLP,\n",
    "    af_body_init_params=af_body_init_params,\n",
    "    af_head_init_params=af_head_init_params,\n",
    "    standardize_outcomes=False\n",
    ")\n",
    "print(gittins_af)\n",
    "\n",
    "gittins_af.save_init('test_ddd')\n",
    "\n",
    "batch_size = 10\n",
    "n_hist = 5\n",
    "n_cand = 2\n",
    "x_hist = torch.rand(batch_size, n_hist, dimension)\n",
    "y_hist = torch.rand(batch_size, n_hist, 1)\n",
    "log_cost_hist = torch.rand(batch_size, n_hist, 1)\n",
    "x_cand = torch.rand(batch_size, n_cand, dimension)\n",
    "log_lambda = torch.rand(batch_size, n_cand, 1)\n",
    "output = gittins_af(x_hist, y_hist, x_cand,\n",
    "                    lambda_cand=log_lambda,\n",
    "                    cost_hist=log_cost_hist,\n",
    "                    is_log=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e226ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.get_info_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dbce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.save_init(\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SaveableObject\n",
    "SaveableObject.load_init(\"fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f613f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Dummy(nn.Module):\n",
    "    def __init__(self, the_module):\n",
    "        super().__init__()\n",
    "        self.the_module = the_module\n",
    "        self.register_forward_pre_hook(self._my_pre_forward_hook)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _my_pre_forward_hook(module, inputs):\n",
    "        # do something\n",
    "        return inputs_modified\n",
    "    \n",
    "    def forward(self, a, b, **kwargs):\n",
    "        return self.the_module(a, b, **kwargs)\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self._dummy = _Dummy()\n",
    "\n",
    "    def forward(self, a, b=3, **kwargs):\n",
    "        return self._dummy(a, b, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a': 1, 'b': 2}\n",
    "for k, v in d.items():\n",
    "    if v == 1:\n",
    "        d[k] = v + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_utils import SaveableObject\n",
    "SaveableObject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Ta(nn.Module):\n",
    "    pass\n",
    "b = Ta()\n",
    "b(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369065e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xf but this version of numpy is 0xe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xf but this version of numpy is 0xe"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2.0\u001b[39m], requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m ei_helper_inverse(x)\n\u001b[0;32m----> 5\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from utils import ei_helper_inverse\n",
    "import torch\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = ei_helper_inverse(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d45f19a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.5743, 0.1013])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import gi_normal\n",
    "import torch\n",
    "\n",
    "c = torch.tensor([0.00001, 0.1])\n",
    "mu = torch.tensor([-0.3, 0.2])\n",
    "sigma = torch.tensor([1.7, 0.06])\n",
    "gi_normal(c, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "456682a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.batch_shape=torch.Size([])\n",
      "posterior.mean.shape=torch.Size([7, 2]), posterior.variance.shape=torch.Size([7, 2])\n",
      "posterior.mean[..., 0].shape=torch.Size([7])\n",
      "posterior.mean.squeeze(-1).shape=torch.Size([7, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aloja/opt/anaconda3/lib/python3.9/site-packages/botorch/models/utils/assorted.py:202: InputDataWarning: Input data is not standardized (mean = tensor([-1.3147, -1.7337]), std = tensor([2.3553, 0.9760])). Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<botorch.posteriors.transformed.TransformedPosterior at 0x7f8c985d40a0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Log\n",
    "\n",
    "batch = 4\n",
    "n_hist = 5\n",
    "d = 6\n",
    "m = 2\n",
    "model = SingleTaskGP(\n",
    "    train_X=torch.rand(n_hist, d),\n",
    "    train_Y=torch.rand(n_hist, m),\n",
    "    outcome_transform=Log()\n",
    ")\n",
    "print(f\"{model.batch_shape=}\")\n",
    "\n",
    "posterior = model.posterior(torch.rand(7, d))\n",
    "print(f\"{posterior.mean.shape=}, {posterior.variance.shape=}\")\n",
    "print(f\"{posterior.mean[..., 0].shape=}\")\n",
    "print(f\"{posterior.mean.squeeze(-1).shape=}\")\n",
    "posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59dfc51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3, 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand([6,3]).unsqueeze(-1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
